{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JN4jOFvLv28"
      },
      "source": [
        "# Import "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaKcD3DULv3K"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "scores = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmDErhaJYeJi"
      },
      "outputs": [],
      "source": [
        "# upload the dataset file (.csv)\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg0zm-5k9wdR"
      },
      "source": [
        "# Processing data (Choose depression + 1 of 4 datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROcPLG1SLv3K"
      },
      "source": [
        "## Depression data (Test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mgtDnAmLv3L"
      },
      "outputs": [],
      "source": [
        "# use pd get the size of the dataset\n",
        "depression = pd.read_csv(\"Depression.csv\")\n",
        "\n",
        "# # of row, # of col\n",
        "print(len(depression.axes[0]), len(depression.axes[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9LUhoQOLv3L"
      },
      "outputs": [],
      "source": [
        "# open the file and read the data\n",
        "f = open('Depression.csv', 'r')\n",
        "rawdata = f.read()\n",
        "f.close()\n",
        "\n",
        "# split the whole data(string) by \\n, and delete the first row(titlea)\n",
        "rawdata = rawdata.split(\"\\n\")\n",
        "rawdata = rawdata[1:100]\n",
        "rawdata = [r.split(',') for r in rawdata]\n",
        "# pop the last row, which is generated by split ','\n",
        "rawdata.pop()\n",
        "\n",
        "# store the features and labels by samples\n",
        "target_features = []\n",
        "target_label = []\n",
        "\n",
        "# store the overall data before split into features and labels\n",
        "sample = []\n",
        "\n",
        "# float or int is the only input format for NN\n",
        "for r in rawdata:\n",
        "    feature = [] \n",
        "    for c in r:\n",
        "      if c !='':\n",
        "        feature.append(float(c))\n",
        "      else:\n",
        "        feature.append(float(0))\n",
        "    sample.append(feature)\n",
        "\n",
        "\n",
        "target_features = np.array([x[:-1] for x in sample])\n",
        "target_label = np.array([x[-1] for x in sample])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-njr30q7Lv3L"
      },
      "outputs": [],
      "source": [
        "x, y = target_features.shape\n",
        "print(target_features.shape)\n",
        "print(target_label.size)\n",
        "add_np = np.random.randint(2, size=(x,13-y))\n",
        "target_features = np.append(target_features, add_np, axis = 1)\n",
        "print(target_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUYa6kckLv3L"
      },
      "source": [
        "## Diabetes data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fmzK7aRLv3M"
      },
      "outputs": [],
      "source": [
        "diabetes = pd.read_csv(\"Diabetes.csv\")\n",
        "\n",
        "print(len(diabetes.axes[0]), len(diabetes.axes[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJH-1hHILv3M"
      },
      "outputs": [],
      "source": [
        "f = open('Diabetes.csv', 'r')\n",
        "rawdata = f.read()\n",
        "f.close()\n",
        "\n",
        "rawdata = rawdata.split(\"\\n\")\n",
        "rawdata = rawdata[1:]\n",
        "rawdata = [r.split(',') for r in rawdata]\n",
        "rawdata.pop()\n",
        "\n",
        "\n",
        "source_features = []\n",
        "source_label = []\n",
        "sample = []\n",
        "\n",
        "for r in rawdata:\n",
        "    feature = [] \n",
        "    for c in r:\n",
        "      if c !='':\n",
        "        feature.append(float(c))\n",
        "      else:\n",
        "        feature.append(float(0))\n",
        "    sample.append(feature)\n",
        "\n",
        "source_features = np.array([x[:-1] for x in sample])\n",
        "source_label = np.array([x[-1] for x in sample])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNyGD11-Lv3M"
      },
      "outputs": [],
      "source": [
        "print(source_features.shape)\n",
        "print(source_label.size)\n",
        "x, y = source_features.shape\n",
        "add_np = np.random.randint(2, size=(x,13-y))\n",
        "source_features = np.append(source_features, add_np, axis = 1)\n",
        "print(source_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BwIrEJhLv3M"
      },
      "source": [
        "## Heart data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hFBhoulLv3N"
      },
      "outputs": [],
      "source": [
        "Heart = pd.read_csv(\"Heart.csv\")\n",
        "\n",
        "print(len(Heart.axes[0]), len(Heart.axes[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fstX9QzbLv3N"
      },
      "outputs": [],
      "source": [
        "f = open('Heart.csv', 'r')\n",
        "rawdata = f.read()\n",
        "f.close()\n",
        "\n",
        "rawdata = rawdata.split(\"\\n\")\n",
        "rawdata = rawdata[1:]\n",
        "rawdata = [r.split(',') for r in rawdata]\n",
        "rawdata.pop()\n",
        "\n",
        "source_features = []\n",
        "source_label = []\n",
        "sample = []\n",
        "\n",
        "for r in rawdata:\n",
        "    feature = [] \n",
        "    for c in r:\n",
        "      if c !='':\n",
        "        feature.append(float(c))\n",
        "    sample.append(feature)\n",
        "\n",
        "source_features = np.array([x[:-1] for x in sample])\n",
        "source_label = np.array([x[-1] for x in sample])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfuJzE76Lv3N"
      },
      "outputs": [],
      "source": [
        "print(source_features.shape)\n",
        "print(source_label.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50d8bfdHYxkT"
      },
      "outputs": [],
      "source": [
        "x, y = source_features.shape\n",
        "add_np = np.random.randint(2, size=(x,13-y))\n",
        "source_features = np.append(source_features, add_np, axis = 1)\n",
        "print(source_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpyYN3mHLv3N"
      },
      "source": [
        "## Breast cancer data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhODcQjmLv3N"
      },
      "outputs": [],
      "source": [
        "Breast = pd.read_csv(\"Breast_cancer_data.csv\")\n",
        "\n",
        "print(len(Breast.axes[0]), len(Breast.axes[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbIyp94-Lv3O"
      },
      "outputs": [],
      "source": [
        "f = open('Breast_cancer_data.csv', 'r')\n",
        "rawdata = f.read()\n",
        "f.close()\n",
        "\n",
        "rawdata = rawdata.split(\"\\n\")\n",
        "rawdata = rawdata[1:]\n",
        "rawdata = [r.split(',') for r in rawdata]\n",
        "rawdata.pop()\n",
        "\n",
        "source_features = []\n",
        "source_label = []\n",
        "sample = []\n",
        "\n",
        "for r in rawdata:\n",
        "    feature = [] \n",
        "    for c in r:\n",
        "      if c !='':\n",
        "        feature.append(float(c))\n",
        "    sample.append(feature)\n",
        "\n",
        "source_features = np.array([x[:-1] for x in sample])\n",
        "source_label = np.array([x[-1] for x in sample])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "641u29q9Lv3O"
      },
      "outputs": [],
      "source": [
        "print(source_features.shape)\n",
        "print(source_label.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLDoywO4Y651"
      },
      "outputs": [],
      "source": [
        "x, y = source_features.shape\n",
        "add_np = np.random.randint(2, size=(x,14-y))\n",
        "source_features = np.append(source_features, add_np, axis = 1)\n",
        "print(source_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Depression data (Train)"
      ],
      "metadata": {
        "id": "ypR5jSMBDtp_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STNN0XoYD4TI"
      },
      "outputs": [],
      "source": [
        "# use pd get the size of the dataset\n",
        "depression = pd.read_csv(\"Depression.csv\")\n",
        "\n",
        "# # of row, # of col\n",
        "print(len(depression.axes[0]), len(depression.axes[1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkw5KFTzD4TJ"
      },
      "outputs": [],
      "source": [
        "# open the file and read the data\n",
        "f = open('Depression.csv', 'r')\n",
        "rawdata = f.read()\n",
        "f.close()\n",
        "\n",
        "# split the whole data(string) by \\n, and delete the first row(titlea)\n",
        "rawdata = rawdata.split(\"\\n\")\n",
        "rawdata = rawdata[101:]\n",
        "rawdata = [r.split(',') for r in rawdata]\n",
        "# pop the last row, which is generated by split ','\n",
        "rawdata.pop()\n",
        "\n",
        "source_features = []\n",
        "source_label = []\n",
        "sample = []\n",
        "\n",
        "# float or int is the only input format for NN\n",
        "for r in rawdata:\n",
        "    feature = [] \n",
        "    for c in r:\n",
        "      if c !='':\n",
        "        feature.append(float(c))\n",
        "      else:\n",
        "        feature.append(float(0))\n",
        "    sample.append(feature)\n",
        "\n",
        "\n",
        "source_features = np.array([x[:-1] for x in sample])\n",
        "source_label = np.array([x[-1] for x in sample])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HST0YKU4D4TK"
      },
      "outputs": [],
      "source": [
        "x, y = source_features.shape\n",
        "print(source_features.shape)\n",
        "print(source_label.size)\n",
        "add_np = np.random.randint(2, size=(x,13-y))\n",
        "source_features = np.append(source_features, add_np, axis = 1)\n",
        "print(source_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "JEdIOr0me42v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqY6P98Hj-9s"
      },
      "source": [
        "## Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymw9-ypDkBzO"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVljSxCtkBzP"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "# input diabetes data\n",
        "model.add(keras.Input(shape = (13,), name='input'))\n",
        "model.add(layers.Dense(20,activation='relu', name='Hid_1'))\n",
        "model.add(layers.Dense(15,activation='relu', name='Hid_2'))\n",
        "model.add(layers.Dense(10,activation='relu', name='Hid_3'))\n",
        "model.add(layers.Dense(2,activation='sigmoid', name='output'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRpTyqiXkBzP"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(source_features, source_label, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz4ChAVHkBzQ"
      },
      "outputs": [],
      "source": [
        "accurary_train = []\n",
        "accurary_test = []\n",
        "sample_train = []\n",
        "sample_test = []\n",
        "label_train = []\n",
        "label_test = []\n",
        "\n",
        "# default value split into 5 folders and repeat 10 times\n",
        "CV = RepeatedKFold()\n",
        "\n",
        "for train_index, test_index in CV.split(source_features):\n",
        "  sample_train,sample_test = source_features[train_index],source_features[test_index]\n",
        "  label_train, label_test = source_label[train_index],source_label[test_index]\n",
        "  accurary_train.append(model.fit(sample_train, label_train))\n",
        "  accurary_test.append(model.fit(sample_test, label_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE4ZPn11kBzR"
      },
      "outputs": [],
      "source": [
        "#LR.fit(target_features,target_label)\n",
        "loss, accuracy = model.evaluate(target_features, target_label)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjuRQxphkBzR"
      },
      "outputs": [],
      "source": [
        "# Extract vectors and apply kmeans\n",
        "print(target_label)\n",
        "output = model.predict(target_features)\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y5quU51kBzS"
      },
      "outputs": [],
      "source": [
        "output = np.array(output)\n",
        "data = pd.DataFrame(output, columns = ['transfer_depression_0','transfer_depression_1'])\n",
        "data['transfer_output'] = np.argmax(output,axis=1)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ-QygIYkBzS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "data.to_csv('depression_kmeans.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot2bR77SZ_1m"
      },
      "source": [
        "## Parallel K-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh8atR36aMAq"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import csv\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "!pip install mpi4py\n",
        "from mpi4py import MPI\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygXqBKqKaQNn"
      },
      "outputs": [],
      "source": [
        "# Devide data set to further scattering\n",
        "def chunkIt(seq, num):\n",
        "    avg = len(seq) / float(num)\n",
        "    out = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(seq):\n",
        "        out.append(seq[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "    return out\n",
        "# Count lables for recalculating means of centroids \t\n",
        "def addCounter(counter1, counter2, datatype):\n",
        "    for item in counter2:\n",
        "        counter1[item] += counter2[item]\n",
        "    return counter1\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size()\n",
        "rank = comm.Get_rank()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZMRTcEw1CQn"
      },
      "outputs": [],
      "source": [
        "\n",
        "global dimensions, num_clusters, num_points,dimensions,data,flag\n",
        "num_clusters=0\n",
        "\n",
        "if rank==0: \t\t\t\t\t\t\t\t\t\t\t\t\t#master part\t\t\t\t\t\t\t\t\t\n",
        "\tnum_clusters = int(2)\n",
        "\tstart_time = time.time() \n",
        " \n",
        "\twith open('depression_kmeans.csv','r') as f:\n",
        "\t\treader = csv.reader(f)\n",
        "\t\tdata = list(reader)\n",
        "\t\t\n",
        "\tdata.pop(0)\n",
        "\tfor i in range (len(data)):\n",
        "\t\tdata[i].pop(0)\n",
        "\tdata=data[0:]\n",
        "\tdata=np.array(data).astype(np.float)\n",
        "\n",
        "\tkmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(data).labels_\n",
        "\t\n",
        "\t# Initialize centroids matrix\n",
        "\tinitial=[]\n",
        "\tfor i in range(num_clusters):\n",
        "\t\tinitial.append(data[i])\n",
        "\tinitial=np.vstack(initial)\n",
        "\n",
        "\t#number of rows\n",
        "\tnum_points = len(data)  \n",
        "  #number of columns                                  \n",
        "\tdimensions = len(data[0])   \n",
        "\n",
        "\t# deviding data set on parts for further scattering\n",
        "\tchunks=chunkIt(data,size)\t\t\t\t\t\t\t\t  \n",
        "\n",
        "#workers part\n",
        "#initilize variable\n",
        "else:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "\tchunks = None\t\t\t\t\t\t\t\t\t\t\t  \n",
        "\tinitial = None\t\t\t\t\t\t\t\t\t\t\t  \n",
        "\tdata = None\t\t\t\t\t\t\t\t\t\t\t     \n",
        "\tdimensions = None\t\t\t\t\t\t\t\t\t\t \n",
        "\tnum_points = None\t\t\t\t\t\t\t\t\t\t  \n",
        "\tcluster= None\t\t\t\t\t\t\t\t\t\t\t \n",
        "\tQ_clust= None\t\t\t\t\t\t\t\t\t\t\t \n",
        "\tnum_clusters= None\t\t\t\t\t\t\t\t\t\n",
        "\tcentroid=None\n",
        "\tkmeans= None\n",
        "\tstart_time=None\n",
        "\t\n",
        "start_time=comm.bcast(start_time,root=0)\n",
        "\n",
        "#send chunks of data set to the workers\n",
        "data=comm.scatter(chunks, root=0)\t\t\t\t\t\t\t  \n",
        "num_clusters=comm.bcast(num_clusters,root=0)\n",
        "\n",
        "#send centroid matrix to the workers\n",
        "initial=comm.bcast(initial, root = 0)\t\t\t\t\t\t  \n",
        "flag= True\n",
        "\n",
        "while flag==True:\n",
        "\tclusters=[]\t\t\t\t\t\t\t\t\t\t\t  \t\t  #initilize variable\n",
        "\tcluster=[]\n",
        "\t# Calculating dist matrix in each process\n",
        "\tdist =np.zeros((len(data),len(initial)))\n",
        "\n",
        "\n",
        "\tfor j in range(len(initial)):\n",
        "\t\tfor i in range(len(data)):\n",
        "\t\t\tdist[i][j]=np.linalg.norm(initial[j]-data[i])\t\n",
        "\t# Initilize lable for each sample in each process\n",
        "\n",
        "\t#iterable take each raw in dist matrix and \n",
        "\tfor i in range (len(dist)):\t\t\t\t\t\t\t\t\t\n",
        "\t\tclusters.append(np.argmin(dist[i])+1)                      \n",
        "\t#print(\"find column index of min value (the index is the number of centorud)\")\n",
        "\t#print(clusters)\n",
        " \n",
        "\t#Calculating the number of samples in each cluster\n",
        "\tQ_clusts=collections.Counter(clusters)\t\n",
        " \t\t\t\t\t\t\n",
        "\t# Summing the number of samples for each cluster\n",
        "\tcounterSumOp = MPI.Op.Create(addCounter, commute=True)\n",
        "\n",
        "\ttotcounter = comm.allreduce(Q_clusts, op=counterSumOp)\n",
        "\tcomm.Barrier()\n",
        "\t#print('Q',Q_clusts)\n",
        "\t#print('T',totcounter)\n",
        "\n",
        "\t# From each worker we gather cluster vector and join them\n",
        "\tcluster=comm.gather(clusters, root=0)\n",
        "\tcomm.Barrier()\n",
        "\tif rank==0:\n",
        "\t\tcluster=[item for sublist in cluster for item in sublist]\n",
        "\n",
        "\n",
        "\tcentroids=np.zeros((len(initial),len(initial[0])))\n",
        "\tfor k in range (1,num_clusters+1):\n",
        "\t\tindices = [i for i, j in enumerate(clusters) if j == k]\n",
        "\t\tcentroids[k-1]=np.divide((np.sum([data[i] for i in indices], axis=0)).astype(np.float),totcounter[k])\n",
        "\n",
        "\tcentroid=comm.allreduce(centroids,MPI.SUM)\n",
        "\tcomm.Barrier()\n",
        "\t#print('centroids',centroids)\n",
        "\t#print ('initial', initial)\n",
        "\t#print('centroid',centroid)\n",
        "\t\t\n",
        "\t\n",
        "\tif np.all(centroid==initial):\n",
        "\t\tflag=False\n",
        "\t\t#print (\"Execution time %s seconds\" % (time.time() - start_time))\n",
        "\t\t\n",
        "\telse:\n",
        "\t#\tprint ('initial after', initial)\n",
        "\t\tinitial= centroid \n",
        "\tcomm.Barrier()\n",
        "\n",
        "\n",
        "if rank==0:\n",
        "\tprint('final clust vect',cluster)\n",
        "\tprint('libkms clust vect',kmeans)\n",
        "\n",
        "\n",
        "\n",
        "\t\t\t\n",
        "\n",
        "\t\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j0Ff3CI-HJQ"
      },
      "outputs": [],
      "source": [
        "cluster_np = np.array(cluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMVGWi408w3J"
      },
      "outputs": [],
      "source": [
        "centroids"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_np"
      ],
      "metadata": {
        "id": "QaTmDZIlIXt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_label"
      ],
      "metadata": {
        "id": "ya9Z76pmIf7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "Vf9cPEsvEdcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel K-Means"
      ],
      "metadata": {
        "id": "q68dXXazEf3W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KElUMSIX-SxC"
      },
      "outputs": [],
      "source": [
        "#data_np = data.to_numpy()\n",
        "plt.scatter(data[:,0], data[:,1], c=cluster_np[:] )\n",
        "plt.scatter(centroids[0][0],centroids[0][1], c = 'red')\n",
        "plt.scatter(centroids[1][0],centroids[1][1], c = 'blue')\n",
        "plt.title(\"Depression_Parallel\")\n",
        "plt.colorbar"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original Labels"
      ],
      "metadata": {
        "id": "_dj4kuOAEi8M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLO_KjRM68rT"
      },
      "outputs": [],
      "source": [
        "plt.scatter(data[:,0], data[:,1], c=target_label )\n",
        "plt.title(\"Depression_Original\")\n",
        "plt.colorbar"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correct number"
      ],
      "metadata": {
        "id": "L-HSRX0YIKTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = 0\n",
        "(nums, values)= data.shape\n",
        "for i in cluster_np:\n",
        "  if cluster_np[i] * 1.0 == target_label[i] +1:\n",
        "    score = score + 1\n",
        "\n",
        "score = score/nums \n",
        "print(\"the accuracy of this model is (Parallel K-Means/Original labels): {}\".format(score))"
      ],
      "metadata": {
        "id": "sTT_zC8JIJwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores.append(score)\n",
        "scores"
      ],
      "metadata": {
        "id": "18eKjA5ZLAkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IUYa6kckLv3L",
        "8BwIrEJhLv3M",
        "rpyYN3mHLv3N",
        "MVYvB0CnkKvv",
        "WZwvxEMNBm3_"
      ],
      "name": "COMP5704_Project",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}